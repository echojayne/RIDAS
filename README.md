# RIDAS: A Multi-Agent Framework for AI-RAN with Representation- and Intention-Driven Agents

RIDAS is a novel multi-agent framework designed for AI-enabled Radio Access Networks (AI-RAN) in the 6G era. It effectively bridges the gap between high-level user intents and low-level network parameter configurations by combining the intent-understanding capabilities of Large Language Models (LLMs) with the fine-grained representation control of user-end agents.

## Abstract

Sixth-generation (6G) networks demand the tight integration of artificial intelligence (AI) into radio access networks (RANs) to meet stringent quality-of-service (QoS) and resource-efficiency requirements. Existing solutions struggle to bridge the gap between high-level user intents and the low-level, parameterized configurations required for optimal performance. To address this challenge, we propose RIDAS, a multi-agent framework composed of representation-driven agents (RDAs) and an intention-driven agent (IDA). RDAs expose an open interface with tunable control parameters—decomposition rank and quantization bits—enabling an explicit trade-off between distortion and transmission rate. The IDA employs a two-stage planning scheme (bandwidth pre-allocation and reallocation) driven by a Large Language Model (LLM) to map user intents and system state into optimal RDA configurations. Experiments demonstrate that RIDAS supports 44.71% more users than the WirelessAgent framework under equivalent QoS constraints. These results validate the ability of RIDAS to capture user intent and allocate resources more efficiently in AI-RAN environments.

## Framework Architecture

The RIDAS workflow follows a clear request-decision-execution cycle:

1. **User Request**: A new user (with an RDA) requests access to the network with a specific service requirement (e.g., low latency, high fidelity).

2. **IDA Decision** (Phase 1: Pre-allocation):
   - The IDA receives the request and queries its internal database for current system status (all user states, total available bandwidth)
   - It consults the experience_configurations table and, using the LLM, generates an optimal initial configuration (quantify_bits, decomposition_rank, code_rate) based on the new user's context (SNR, acceptance of distortion).
   - It calculates the bandwidth required for this configuration.

3. **Resource Validation & Decision** (Phase 2: Reallocation):
   - If bandwidth is sufficient: The IDA allocates the resources, updates the user record database, and admits the user.
   - If bandwidth is insufficient: The IDA initiates the reallocation process. It prompts the LLM to analyze all currently connected users and identify candidates for a "downgrade" (i.e., using a more resource-efficient configuration that still meets their QoS floor).
   - If reallocation can free up enough bandwidth for the new user, the IDA applies the changes and admits the new user. Otherwise, the access request is denied.
4. R**DA Execution**: The user-side RDA receives the configuration parameters from the IDA and processes/transmits its data accordingly.

## File Structure

```markdown
├── RIDAS/
│   ├── clip/                 # Files related to the CLIP model.
│   ├── Tables/
│   │   ├── Memory/
│   │   │   ├── experience_configurations.csv  # (Generated by RDA.py) The core experience database for IDA decisions.
│   │   │   ├── Memory.db                     # (Generated by RIDAS.py) The SQLite database file.
│   │   │   └── user_record.csv               # (Generated by RIDAS.py) Records of currently active user configurations.
│   │   └── User_Records/           # (Generated by RIDAS.py) Historical snapshots of user records.
│   ├── user_configs.json       # User queue defining incoming user requests for the simulation.
│   ├── config.json             # The main project configuration file (API keys, paths, etc.).
│   ├── LICENSE                 # Project license.
│   ├── prompts.py              # Contains the prompt templates for interacting with the LLM.
│   ├── RDA.py                  # Implements the RDA logic and generates the experience database.
│   ├── README.md               # This file.
│   └── RIDAS.py                # Implements the IDA logic and the main system simulation.
└── ...
```

## Installation and Configuration

1. Clone the Repository

```bash
git clone https://github.com/echojayne/RIDAS.git
cd RIDAS
```

2. Install Dependencies

​	It is recommended to use a virtual environment. Install the required libraries using pip:

```bash
pip install torch torchvision pandas tqdm openai
pip install git+https://github.com/openai/CLIP.git
```

3. Configure Your Environment

   Open config.json and modify it according to your setup.

   - **CRITICAL**: API Key: You must provide your Large Language Model API key and base URL in the IDA.LLM section. The current configuration is set for DeepSeek.

     ```json
     "LLM":{
       "base_url": "https://api.deepseek.com/v1",
       "API_KEY": "YOUR_API_KEY_HERE",
       "model": "deepseek-chat",
       "temperature": 0.1,
       "max_tokens": 15000,
       "top_p": 1.0
     }
     ```

   - **Dataset Path**: In the RDA section, ensure dataset_path points to the directory where you want to store the CIFAR dataset. The script will download it automatically if not found.

   - **Device**: Set the device (e.g., "cuda:0" or "cpu") for PyTorch.

4. Prepare the User Queue

   The simulation in RIDAS.py reads a sequence of user requests from the JSON file specified by user_queue in config.json (i.e., Tables/user_configs.json). Ensure this file exists and is correctly formatted. Example:

   ```json
   // Tables/user_configs.json
   [
     {
       "user_id": 1,
       "snr": 15.0,
       "max_transmission_latency": 0.0005,
       "acceptance_of_distortion": "low"
     },
     {
       "user_id": 2,
       "snr": 25.0,
       "max_transmission_latency": 0.0002,
       "acceptance_of_distortion": "medium"
     }
   ]
   ```

## How to Run

The execution of RIDAS is a two-step process:

### Step 1: Generate the Experience Database

The IDA's intelligent decisions rely on a pre-computed experience database that maps RDA configurations to performance. Run RDA.py to generate this file first.

```bash
python RDA.py
```

This script will:

- Load the pre-trained ViT-B/16 CLIP model and the CIFAR-10 dataset.

- Iterate through all combinations of rank_list ([0, 1, 2, 4, 8]) and quantization_bit_list ([1, 2, 3, 4, 5, 6, 7, 8]) defined in config.json.

- For each combination, evaluate the average bitstream length and task accuracy over the test set.

- Save the results to the path specified by experience_configuration_csv_file_path (i.e., Tables/Memory/experience_configurations.csv).

**Note**: This process may take some time as it involves multiple evaluation passes over the entire dataset.

### Step 2: Run the Main IDA Simulation

Once the experience database is generated, you can run the main simulation script.

```bash
python RIDAS.py
```

This script will:

- Initialize the system and load experience_configurations.csv into an in-memory SQLite database.

- Read user requests one by one from the user queue file (Tables/user_configs.json).

- For each user, execute the IDA's two-stage planning process, including interaction with the LLM.

- Simulate the user's admission or rejection and update the system state (total bandwidth used, number of active users).

- Log the entire process to the console and save snapshots of the user records to the Tables/User_Records/ directory.

## Reference

OpenAI CLIP: [https://github.com/openai/CLIP](https://github.com/openai/CLIP)

## How to Cite

If you use this work in your research, please cite our paper:

```bibtex
@misc{RIDAS-AI-RAN,
	title={RIDAS: A Multi-Agent Framework for AI-RAN with Representation- and Intention-Driven Agents},
	author={},
	year={2025},
	eprint={},
	archivePrefix={arXiv},
	primaryClass={cs.NI},
	url={https://github.com/echojayne/RIDAS.git}
}
```